# -*- coding: utf-8 -*-
"""Submission - Predictive Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b5VNSydF4eDMeT5qUztoBpNlJd-zP-Oj

# **Data Loading**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

# load the dataset
url = '//content/drive/MyDrive/Colab Notebooks/Machine Learning 4/1553768847-housing.csv'
housing = pd.read_csv(url)
housing

housing.shape

"""Output kode di atas memberikan informasi sebagai berikut:

- Ada 20.640 baris records dalam dataset.
- Terdapat 10 kolom yaitu: longitude, latitude,	housing_median_age,	total_rooms,	total_bedrooms,	population,	households,	median_income,	ocean_proximity, dan	median_house_value

# **Exploratory Data Analysis**

## Deskripsi Variabel

- longitude : posisi bujur
- latitude : posisi lintang
- housing_median_age : usia rata-rata rumah
- total_rooms : jumlah kamar
- total_bedrooms : jumlah kamar tidur
- population : populasi
- households : jumlah rumah tangga
- median_income : median pendapatan
- ocean_proximity : kedekatan terhadap laut
- median_house_value : median harga rumah
"""

housing.info()

"""Dari output terlihat bahwa:
- Terdapat 1 kolom dengan tipe object, yaitu: ocean_proximity. Kolom ini merupakan categorical features (fitur non-numerik).
- Terdapat 4 kolom numerik dengan tipe data float64 yaitu: longitude, latitude,	total_bedrooms, dan	median_income.
- Terdapat 5 kolom numerik dengan tipe data int64, yaitu: housing_median_age,	total_rooms,vpopulation,	households, dan	median_house_value. Kolom median_house_value merupakan target fitur.
"""

housing.describe()

"""Fungsi describe() memberikan informasi statistik pada masing-masing kolom, antara lain:

- Count  adalah jumlah sampel pada data.
- Mean adalah nilai rata-rata.
- Std adalah standar deviasi.
- Min yaitu nilai minimum setiap kolom.
- 25% adalah kuartil pertama. Kuartil adalah nilai yang menandai batas interval dalam empat bagian sebaran yang sama.
- 50% adalah kuartil kedua, atau biasa juga disebut median (nilai tengah).
- 75% adalah kuartil ketiga.
- Max adalah nilai maksimum.

## Data Assesing

### Data Duplikat
"""

housing.duplicated().sum()

"""### Missing Value"""

housing.isna().sum()

"""## Data Cleaning

### Imputation

Karena terdapat 207 data null maka perlu dilakukan imputasi dengan mengganti nilai null dengan nilai mean
"""

housing['total_bedrooms'] = housing['total_bedrooms'].fillna(housing['total_bedrooms'].mean())

housing.isna().sum()

"""###Menangani Outliers

Beberapa pengamatan dalam satu set data kadang berada di luar lingkungan pengamatan lainnya. Pengamatan seperti itu disebut outlier.

Ada beberapa teknik untuk menangani outliers, antara lain:
- Hypothesis Testing
- Z-score method
- IQR Method

Pada kasus ini, Anda akan mendeteksi outliers dengan teknik visualisasi data (boxplot). Kemudian, Anda akan menangani outliers dengan teknik IQR method

IQR = Inter Quartile Range

IQR = Q3 - Q1.
"""

#Seltman dalam “Experimental Design and Analysis” [24] menyatakan bahwa outliers yang diidentifikasi oleh boxplot (disebut juga “boxplot outliers”) didefinisikan sebagai data yang nilainya 1.5 QR di atas Q3 atau 1.5 QR di bawah Q1.
#Hal pertama yang perlu Anda lakukan adalah membuat batas bawah dan batas atas. Untuk membuat batas bawah, kurangi Q1 dengan 1,5 * IQR. Kemudian, untuk membuat batas atas, tambahkan 1.5 * IQR dengan Q3.

housing1=housing.select_dtypes(exclude=['object'])
for column in housing1:
        plt.figure()
        sns.boxplot(data=housing1, x=column)

Q1 = housing.quantile(0.25)
Q3 = housing.quantile(0.75)
IQR=Q3-Q1

housing = housing[~((housing<(Q1-1.5*IQR))|(housing>(Q3+1.5*IQR))).any(axis=1)]

"""Cek ukuran dataset setelah drop outliers dengan
housing.shape
"""

housing.shape

"""Dataset Anda sekarang telah bersih dan memiliki 17609 sampel.

## Data Analysis

Selanjutnya, lakukan proses analisis data dengan teknik Univariate EDA. Pertama, Anda bagi fitur pada dataset menjadi dua bagian, yaitu numerical features dan categorical features.
"""

categorical_features = ['ocean_proximity']
numerical_features = ['longitude', 'latitude',	'housing_median_age',	'total_rooms',	'total_bedrooms',	'population',	'households',	'median_income', 'median_house_value']

"""### Univariate Analysis

Univariate visualization merupakan bentuk visualisasi data yang hanya merepresentasikan informasi yang terdapat pada satu variabel. Jenis visualisasi ini umumnya digunakan untuk memberikan gambaran terkait distribusi sebuah variabel dalam suatu dataset.

a. Data Kategori
"""

feature = categorical_features[0]
count = housing[feature].value_counts()
percent = 100*housing[feature].value_counts(normalize=True)
df = pd.DataFrame({'Jumlah sampel':count, 'Persentase':percent.round(1)})
print(df)
count.plot(kind='bar', title=feature);

"""b. Data Numerik"""

housing.hist(bins=50, figsize=(20,15))
plt.show()

"""Dari histogram "median_house_value", diperoleh beberapa informasi, antara lain:
- Peningkatan harga median rumah sebanding dengan penurunan jumlah sampel. Hal ini dapat terlihat jelas dari histogram "median_house_value" yang grafiknya mengalami penurunan seiring dengan semakin banyaknya jumlah sampel (sumbu x).
- Distribusi harga miring ke kanan (right-skewed). Hal ini akan berimplikasi pada model.

### Multivariate Analysis

Multivariate visualization merupakan jenis visualisasi data untuk menggambarkan informasi yang terdapat dalam lebih dari dua variabel. Jenis visualisasi ini digunakan untuk merepresentasikan hubungan dan pola yang terdapat dalam multidimensional data.

Pada tahap ini,cek rata-rata "median_house_value" terhadap masing-masing fitur untuk mengetahui pengaruh fitur kategori terhadap "median_house_value".

a. Data Kategori
"""

cat_features = housing.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y='median_house_value', kind="bar", dodge=False, height = 4, aspect = 3,  data=housing, palette="Set3")
  plt.title("Rata-rata 'median_house_value' relatif terhadap - {}".format(col))

"""Dengan mengamati rata-rata 'median_house_value' relatif terhadap fitur kategori di atas, diperoleh *insight* sebagai berikut:

- Pada fitur 'ocean_proximity', rata-rata 'median_house_value' cenderung bervariasi. Rentangnya berada antara 120000 hingga 400000.
- Nilai 'median_house_value' tertinggi berada pada nilai 'ocean_proximity' yaitu 'ISLAND' dan nilai 'median_house_value' terendah berada pada nilai 'ocean_proximity' yaitu 'INLAND'. Sehingga, fitur 'ocean_proximity' memiliki pengaruh yang signifikan terhadap rata-rata 'median_house_value'.
- Kesimpulan akhir, fitur kategori memiliki pengaruh terhadap 'median_house_value'.

b. Data Numerik

Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
"""

sns.pairplot(housing, diag_kind = 'kde')

"""Fungsi pairplot dari library seaborn menunjukkan relasi pasangan dalam dataset. Dari grafik, terlihat plot relasi masing-masing fitur numerik pada dataset. Pada pola sebaran data grafik pairplot sebelumnya, terlihat bahwa 'median_income' memiliki korelasi dengan fitur 'median_house_value'. Sedangkan kedua fitur lainnya  terlihat memiliki korelasi yang lemah karena sebarannya tidak membentuk pola

Koefisien korelasi berkisar antara -1 dan +1. Ia mengukur kekuatan hubungan antara dua variabel serta arahnya (positif atau negatif). Mengenai kekuatan hubungan antar variabel, semakin dekat nilainya ke 1 atau -1, korelasinya semakin kuat. Sedangkan, semakin dekat nilainya ke 0, korelasinya semakin lemah

Untuk mengevaluasi skor korelasinya, gunakan fungsi corr().
"""

plt.figure(figsize=(10, 8))
correlation_matrix = housing.corr().round(2)
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Jika diamati, fitur 'median_income' memiliki skor korelasi yang cukup besar (0.63) dengan fitur target 'median_house_value'. Artinya, fitur 'median_house_value' berkorelasi cukup tinggi dengan keempat fitur tersebut. Sementara itu, fitur lainnya memiliki korelasi negatif sehingga, fitur tersebut dapat di-drop."""

housing.drop(['longitude', 'latitude','population'], inplace=True, axis=1)
housing.head()

"""# **Data Preparation**

Data Peparation merupakan tahapan penting dalam proses pengembangan model machine learning. Pada tahap ini lakukan proses transformasi pada data sehingga menjadi bentuk yang cocok untuk proses pemodelan. Ada beberapa tahapan yang umum dilakukan pada data preparation, antara lain, seleksi fitur, transformasi data, feature engineering, dan dimensionality reduction.

Pada bagian ini lakukan empat tahap persiapan data, yaitu:
- Encoding fitur kategori.
- Reduksi dimensi dengan Principal Component Analysis (PCA).
- Pembagian dataset dengan fungsi train_test_split dari library sklearn.
- Standarisasi.

## Encoding

Untuk melakukan proses encoding fitur kategori, salah satu teknik yang umum dilakukan adalah teknik one-hot-encoding.  Lakukan proses encoding ini dengan fitur get_dummies.
"""

from sklearn.preprocessing import  OneHotEncoder
housing = pd.concat([housing, pd.get_dummies(housing['ocean_proximity'], prefix='ocean_proximity')],axis=1)
housing.drop(['ocean_proximity'], axis=1, inplace=True)
housing.head()

"""##Reduksi Dimensi dengan PCA

Teknik reduksi (pengurangan) dimensi adalah prosedur yang mengurangi jumlah fitur dengan tetap mempertahankan informasi pada data. Teknik pengurangan dimensi yang paling populer adalah Principal Component Analysis atau disingkat menjadi PCA. Ia adalah teknik untuk mereduksi dimensi, mengekstraksi fitur, dan mentransformasi data dari “n-dimensional space” ke dalam sistem berkoordinat baru dengan dimensi m, di mana m lebih kecil dari n.
"""

sns.pairplot(housing[['housing_median_age',	'total_rooms',	'total_bedrooms',	'households']], plot_kws={"s": 4});

"""Berdasarkan hasil visualisasi dapat diketahui yang memiliki hubungan antar fitur hanya tiga yaitu 'total_rooms',	'total_bedrooms',	'households'"""

sns.pairplot(housing[['total_rooms',	'total_bedrooms',	'households']], plot_kws={"s": 3});

"""Selanjutnya, reduksi 3 fitur ini dengan PCA


"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3, random_state=123)
pca.fit(housing[['total_rooms',	'total_bedrooms',	'households']])
princ_comp = pca.transform(housing[['total_rooms',	'total_bedrooms',	'households']])

"""Setelah menerapkan class PCA, cek proporsi informasi dari ketiga komponen PCs tadi."""

pca.explained_variance_ratio_.round(3)

"""Dari output di atas 98.5% informasi pada ketiga fitur 'total_rooms',	'total_bedrooms',	'households' terdapat pada PC pertama. Sedangkan sisanya, sebesar 1.4% dan 0.1% terdapat pada PC kedua dan ketiga.

Berdasarkan hasil ini, reduksi fitur (dimensi) dan hanya mempertahankan PC (komponen) pertama saja. PC pertama ini akan menjadi fitur 'house properties' menggantikan ketiga fitur lainnya ('total_rooms',	'total_bedrooms',	'households'). Beri nama fitur ini 'house properties'
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=1, random_state=123)
pca.fit(housing[['total_rooms',	'total_bedrooms',	'households']])
housing['house properties'] = pca.transform(housing.loc[:, ('total_rooms',	'total_bedrooms',	'households')]).flatten()
housing.drop(['total_rooms',	'total_bedrooms',	'households'], axis=1, inplace=True)
housing.head()

"""##Train-Test-Split

Membagi dataset menjadi data latih (train) dan data uji (test) merupakan hal yang harus diakukan sebelum membuat model. Hal ini diperlukan untuk menguji seberapa baik generalisasi model terhadap data baru.

Pada model ini, proporsi pembagian sebesar 90:10 dengan fungsi train_test_split dari sklearn.
"""

from sklearn.model_selection import train_test_split

X = housing.drop(["median_house_value"],axis =1)
y = housing["median_house_value"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""##Standarisasi

Algoritma machine learning memiliki performa lebih baik dan lebih cepat ketika dimodelkan pada data dengan yang mendekati distribusi normal. Scaling dan standarisasi merupakan metode yang dapat dilakukan

Untuk fitur numerik, tidak dilakukan proses transformasi dengan one-hot-encoding seperti pada fitur kategori. Yang digunakan untuk standarisasi adalah StandarScaler.

StandardScaler merupakan proses standarisasi fitur dengan mengurangkan mean kemudian membaginya dengan standar deviasi untuk menggeser distribusi.  StandardScaler menghasilkan distribusi dengan standar deviasi sama dengan 1 dan mean sama dengan 0.
"""

from sklearn.preprocessing import StandardScaler

numerical_features = ['housing_median_age',	'median_income', 'house properties']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""Seperti yang telah disebutkan sebelumnya, proses standarisasi mengubah nilai rata-rata (mean) menjadi 0 dan nilai standar deviasi menjadi 1. Untuk itu perlu dicek nilai mean dan standar deviasi pada setelah proses standarisasi."""

X_train[numerical_features].describe().round(4)

"""Berdasarkan tabel di atas, sekarang nilai mean = 0 dan standar deviasi = 1.

# **Model Development**

Model development adalah tahapan dimana digunakan algoritma machine learning untuk menjawab problem statement dari tahap business understanding
Pada tahap ini, dibuat model machine learning dengan tiga algoritma. Kemudian, evaluasi performa masing-masing algoritma dan pilih algoritma mana yang memberikan hasil prediksi terbaik. Ketiga algoritma yang akan digunakan, antara lain:
1. Regresi Linier
2. Regresi Ridge
3. Random Forest
3. Random Forest dengan Tuning GridSearchCV
"""

#Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['LinearRegression', 'RidgeRegression', 'RandomForest', 'RandomForest_GridSearchCV'])

"""##1. Regresi Linear

Regresi linear adalah teknik analisis data yang memprediksi nilai data yang tidak diketahui dengan menggunakan nilai data lain yang terkait dan diketahui.
"""

#Selanjutnya, untuk melatih data dengan KNN, tuliskan code berikut.
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, r2_score, mean_absolute_error, mean_squared_error

LR = LinearRegression()
LR.fit(X_train, y_train)
print ('Coefficients: ', LR .coef_)
print ('Intercept: ', LR .intercept_)
models.loc['train_mse','Linear Regression'] = mean_squared_error(y_pred = LR.predict(X_train), y_true=y_train)

"""Meskipun regresi linear sederhana, mudah dipahami, dan mudah digunakan, ia memiliki kelemahan karena hasil regresi merupakan hasil ramalan dari analisis regresi merupakan nilai estimasi sehingga kemungkinan untuk tidak sesuai dengan data aktual

##2. Ridge Regression

Regresi Ridge merupakan metode estimasi koefisien regresi yang diperoleh melalui penambahan konstanta bias sehingga diperoleh persamaan regresi linier yang baru dan tidak mengandung multikolinieritas
"""

from sklearn.linear_model import Ridge
RR = Ridge()
RR.fit(X_train, y_train)
models.loc['train_mse','Ridge Regression'] = mean_squared_error(y_pred = RR.predict(X_train), y_true=y_train)

"""##3. Random Forest

Algoritma random forest dapat digunakan untuk menyelesaikan masalah klasifikasi dan regresi. Random forest juga merupakan algoritma yang sering digunakan karena cukup sederhana tetapi memiliki stabilitas yang mumpuni.
"""

#Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor

#buat model prediksi
RF1 = RandomForestRegressor(random_state=100)
RF1.fit(X_train, y_train)
models.loc['train_mse','Random Forest'] = mean_squared_error(y_pred=RF1.predict(X_train), y_true=y_train)

"""##4. Random Forest dengan Tuning GridSearchCV

Untuk meningkatkan model, dilakukan eksperimen dengan menggunakan GridSearchCV untuk melakukan hyperparameter tuning pada Random Forest

Berikut adalah hyperparameter yang digunakan:

- n_estimator: jumlah trees (pohon) di forest
- max_depth: kedalaman atau panjang pohon. Ia merupakan ukuran seberapa banyak pohon dapat membelah (splitting) untuk membagi setiap node ke dalam jumlah pengamatan yang diinginkan.
- min_samples_split menentukan jumlah minimum sampel yang diperlukan untuk memisahkan simpul internal
- min_samples_leafmenentukan jumlah minimum sampel yang diperlukan untuk berada di simpul daun
"""

params = {'n_estimators' : [50,80,100],
          'max_depth' : [3,5,10],
           'min_samples_split':[2,3,4],
            'min_samples_leaf': [2,3,4]}

from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(estimator= RF1 , param_grid=params, cv=3, scoring='r2')
grid.fit(X_train, y_train)

grid.best_params_

RF2 = RandomForestRegressor(max_depth =10,min_samples_leaf = 4,min_samples_split = 2,n_estimators = 100,random_state=100)
RF2.fit(X_train, y_train)
models.loc['train_mse','Random Forest'] = mean_squared_error(y_pred=RF2.predict(X_train), y_true=y_train)

"""# **Evaluasi Model**

Sekarang, setelah model selesai dilatih dengan 4 algoritma, selanjutnya lihat performa model dengan menggunakan metrik evaluasi
"""

def metrics(name,key,arg):
    print('Name of the model: ',name)
    print('R^2 of the model:',r2_score(key,arg))
    print('MSE of the model:',np.sqrt(mean_squared_error(key,arg)))
    print('MAE of the model:',mean_absolute_error(key,arg))
    print('......')

y_LR = LR.predict(X_test)
y_RR = RR.predict(X_test)
y_RF1 = RF1.predict(X_test)
y_RF2 = RF2.predict(X_test)

metrics('Performa Model 1', y_test, y_LR)
metrics('Performa Model 2', y_test, y_RR)
metrics('Performa Model 3', y_test, y_RF1)
metrics('Performa Model 4', y_test, y_RF2)

"""Sekarang, setelah model selesai dilatih, lakukan proses scaling terhadap data uji. Hal ini harus dilakukan agar skala antara data latih dan data uji sama dan bisa dilakukan evaluasi.

Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""Selanjutnya, evaluasi ketiga model yang ada dengan metrik MSE yang telah dijelaskan di atas."""

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['LR','RR','RF1', 'RF2'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'LR': LR, 'RR': RR, 'RF1': RF1, 'RF2': RF2}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

"""Untuk memudahkan, plot metrik tersebut dengan bar chart. Implementasikan kode di bawah ini:"""

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""Untuk mengujinya, buat prediksi menggunakan beberapa harga dari data test."""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Terlihat bahwa prediksi dengan Random Forest (RF), baik RF1 ataupun RF2 memberikan hasil yang paling mendekati y_true."""